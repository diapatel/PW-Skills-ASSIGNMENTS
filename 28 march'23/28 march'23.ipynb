{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a116251-0b6b-4fd0-b910-a869e7839c4b",
   "metadata": {},
   "source": [
    "Q1 . What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "\n",
    "Q2. What are the assumptions of Ridge Regression?\n",
    "\n",
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "\n",
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "\n",
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "\n",
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "\n",
    "Q7. How do you interpret the coefficients of Ridge Regression?\n",
    "\n",
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6adeeb8-22d6-4c47-be0b-01d94c07d358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b32c7c34-db18-46d6-be15-fb5474528221",
   "metadata": {},
   "source": [
    "* Ridge regression is a regularization technique used in Linear Regression to mitigate overfitting and multicollinearity\n",
    "* in OLS, we try to minimize the loss function. but it is sensitive to multicollinearity. this gives unstable coefficient estimates.\n",
    "* Ridge Regression adds a penalty term to the loss function of OLS, that not only minimizes the loss function but also keeps the values of coefficients as small as possible.\n",
    "* lambda is a tuning parameter which controls the amount of shrinking of the coefficients. higher value of lambda leads to a higher shrinkage, which reduces the impact of each coefficient and as lambda approaches zero, Ridge Regression approaches OLS.\n",
    "* The aim of Ridge Regression is to find more stable and reliable values of coefficients. it trades off some amount of bias for  reducing variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6ce20-a481-40bc-9802-f80bcd00c5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44694554-3094-4a22-b115-4f989095a6d6",
   "metadata": {},
   "source": [
    "### SOLUTION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af8b9a7-16f0-44c2-aa6c-6e5ce7ba9fb6",
   "metadata": {},
   "source": [
    "* <b>LINEARITY</b>: independent and dependent variables are linearly related.\n",
    "* <b>INDEPENDENCE OF OBSERVATIONS</b>: observations are independent of each other.\n",
    "* <b>NORMALITY</b> :The residuals or errors follow normal distribution. important for statistical inferences and hypothesis testing\n",
    "* <b>HOMOSCEDASTICITY</b> : the variance of residuals is constant across all levels of independent variables. in other words, the spread of residuals should be consistent across all levels of independent variables.\n",
    "* <b>NO PERFECT MULTICOLLINEARITY</b>: Ridge regression assumes that there is no perfect multicollinearity among the independent variables. Perfect multicollinearity occurs when an independent variable can be exactly predicted by a linear combination of other independent varibles. Ridge Regression helps to address multicollinearity byshrinking the coefficients but can't effectively address perfect multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab0705-85d3-4f01-969f-138df221fe28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "733e1295-3696-43db-8e75-c68960cfd190",
   "metadata": {},
   "source": [
    "### SOLUTION 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50c636-6a8f-44b1-8b79-44a8aa23973b",
   "metadata": {},
   "source": [
    "A suitable value of lambda should strike a balance between reducing multicollinearity and maintaining the model's predictive power.\n",
    "\n",
    "* <b>CROSS-VALIDATION</b>: the dataset is divided into multiple subsets(k-folds). the model is trained on different combinations of subsets and its performance is evaluated based on the remaining subset. this process is repeated for different values of lambda and the one that give best performance metric (MSE, R2-score) is selected.\n",
    "\n",
    "* <b>BAYESIAN METHODS</b> : these incorporate prior information about the lambda, like its distribution or expected range and then update its value based on the data.\n",
    "\n",
    "* <b>DOMAIN KNOWLEDGE</b> : Domain knowledge can be used to determine the value of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0c8a3-6f30-463a-a894-541b9e89a926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffaef6a6-979e-4b3f-917f-1dbb186956be",
   "metadata": {},
   "source": [
    "### SOLUTION 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a6a55-f09d-4eab-b302-21528986ffb8",
   "metadata": {},
   "source": [
    "* Ridge Regression can shrink the values of coefficients towards zero, but doesn't make them exactly zero like Lasso Regression, so it performs feature selection only to some extent.\n",
    "\n",
    "* Ridge introduces a penalty term that has a tuning parameter lambda. it shrinks the values of coefficients.\n",
    "* because of this shrinkage, the less important features will have lower weights. but they wouldn't exactly be zero, so we cant perform actual feature selection where we select only a subset of important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a94fbb-82f5-4885-9609-a4f979a9bd96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f54e4da-de69-4d19-a1cd-892a805d80c6",
   "metadata": {},
   "source": [
    "### SOLUTION 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb900ab-d04f-4d0c-9fa1-fa1ad6336249",
   "metadata": {},
   "source": [
    "* in case of linear regression, multicollinearity leads to high variance in the cofficient estimates, whereas Ridge shrinks the values of coefficients and makes them less sensitives to small changes in data , thus making them more stable\n",
    "\n",
    "* even though the coefficients are shrinked, all the features are retained, even though they are less important or highly correlated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5ecb2-98d1-4fdd-8fc2-91b218689f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b6e2b40-b9f7-4856-92ae-c8e2a1d9039e",
   "metadata": {},
   "source": [
    "### SOLUTION 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f97948-153d-4f76-b7a0-9385e8c1d5a0",
   "metadata": {},
   "source": [
    "Categorical variables have to be encoded to be used in Ridge. this is done by :<Br>\n",
    "1 ONE HOT ENCODING : converts each category into a binary column. eg, if there are 3 categories in Color column: Red, Green, blue: Color_red, Color_Green, color_blue, where presence of each category is  represented by 0 or 1.\n",
    "\n",
    "2 DUMMY VARIABLE ENCODING: it is like one hot encoding but uses one less column. it uses one category as a reference category , and makes columns for the rest of the categories. eg, if red is the reference category, we'd have 2 binary features, color_green, color_blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7179d-c500-4cb3-b5bd-96f1c136d72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2b91a23-548c-48f9-b217-845027c55eca",
   "metadata": {},
   "source": [
    "### SOLUTION 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089bfa6d-a7ed-4c1d-abe4-6b18c667e8d1",
   "metadata": {},
   "source": [
    "* THe values of coefficients in case of Ridge are generally smaller than OLS. Following factors should be kept in mind while interpreting Ridge  coefficients:\n",
    "\n",
    "1. MAGNITUDE: The magnitude of coefficient represents the strength of relation between that independent variable and the dependent variable. Ridge coefficients are generally smaller than OLS\n",
    "\n",
    "2. RELATIVE IMPORTANCE: the coefficients can be compared within the same Ridge model because all of them have been shrunk by the same prorportion.\n",
    "\n",
    "3. DIRECTION: If coefficient has a + sign, that independent variable is positively related to the dependent variable. and if the coeffiecient has a negative sign, then it is negatively related to the dependent variable.\n",
    "\n",
    "4. Contextual Understanding: The interpretation of coefficients should always be done in the context of the specific problem and the variables involved. Consider the units of the variables and their practical implications. For example, if the dependent variable represents sales revenue, a coefficient of a particular independent variable can be interpreted as the change in revenue associated with a one-unit change in that independent variable, all else being equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b79dddd-10b9-418e-8f17-ce7dbf6de881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06f6af09-70a2-4143-9643-7bb1a3f65a92",
   "metadata": {},
   "source": [
    "### SOLUTION 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdee8c1-c7e6-4f47-bd41-9baa59f8b753",
   "metadata": {},
   "source": [
    "Yes, Ridge regression can be used in time series analysis, for feature selection, training the model, model evalutation,etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88e18a-76c6-466a-9df8-dfd837161a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
