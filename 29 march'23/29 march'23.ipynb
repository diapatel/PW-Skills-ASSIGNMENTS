{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064c20b1-6655-4b2e-b847-8943651b448e",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?<br>\n",
    "\n",
    "Q2. What is the main advantage of using Lasso Regression in feature selection?<Br>\n",
    "\n",
    "Q3. How do you interpret the coefficients of a Lasso Regression model?<Br>\n",
    "\n",
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the \n",
    "model's performance?<Br>\n",
    "\n",
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?<Br>\n",
    "\n",
    "Q6. What is the difference between Ridge Regression and Lasso Regression?<br>\n",
    "\n",
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?<Br>\n",
    "\n",
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828a6e8-881d-44db-8b83-245355a67477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b457849-2396-4f27-b383-8e310635958f",
   "metadata": {},
   "source": [
    "### SOLUTION 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb0c63-809e-429e-888e-677a98fbcbdb",
   "metadata": {},
   "source": [
    "* Lasso (Least Absolute Shrinkage and Selection Optimizer) regression is a kind of regularization technique used to deal with multicollinearity and overfitting.\n",
    "* It adds a penalty term to the loss function using L1 regulariation. the penalty term is the product of lambda and the absolute value of coefficients.\n",
    "* it helps in feature selection because unlike ridge, it can shrink the values of less important or irrelevant features to zero.\n",
    "* while performing feature selection, it also introduces sparsity in the data and eliminates irrelevant features. this reduces the complexity of the model (and makes interpretation easier) and also, reduces chances of overfitting.\n",
    "* the tuning parameter lambda controls the shrinkage of coefficients and also the sparsity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3f75f-56bf-41f3-95fe-49760cb27156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e25fa4e6-e00c-4478-89f3-4350be0d2de1",
   "metadata": {},
   "source": [
    "### SOLUTION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f686c0-e2e0-4461-b8c8-5fa1c66a6860",
   "metadata": {},
   "source": [
    "The main advantage of using lasso regression in feature selection is that it shrinks the values of coefficients irrelevant or less important features to zero and thus, along with regularization, it also helps in dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5039d-9a04-4dbf-80f6-0eb941df0668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e512782-8e6b-48b0-bdef-5d3a0a8e147a",
   "metadata": {},
   "source": [
    "### SOLUTION 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c485b5b-cb04-4207-b8a6-53253e4d1873",
   "metadata": {},
   "source": [
    "* The magnitude of coefficient of an independent variable represents the contribution of that variable in predicting the value of dependent variable.\n",
    "* we cannot compare the values of lasso coefficients to Linear regression coefficients because in lasso, the coefficient values are shrunk\n",
    "* however, the values of lasso coefficients can be compared within themselves to interpret which features have a higher contribution in predicting the value of dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde09133-a274-4810-b2a0-ac756a86ab35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bccf4d70-3eba-42be-a65d-3e059a65bb76",
   "metadata": {},
   "source": [
    "### SOLUTION 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8cb9c7-2f67-4de7-b711-c7a908c69841",
   "metadata": {},
   "source": [
    "* Lambda is the tuning parameter in lasso regression\n",
    "* it controls the shrinkage of coefficients and consequently , sparsity of the dataset.\n",
    "* higher the value of lambda, higher is the shrinkage\n",
    "* it introduces bias to the data , thus reduces the variance. it helps in reducing overfitting\n",
    "* a value of lambda has to be chosen such that it optimally balances the bias-variance and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836605c2-6bdc-467b-9f08-e55697bfc80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45268392-9ed5-416b-ab12-f19d22d3e82a",
   "metadata": {},
   "source": [
    "### SOLUTION 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66420005-f98e-4f67-a6cf-40f8d976c42e",
   "metadata": {},
   "source": [
    "* Lasso is designed for linear data, where there is a linear relationship between independent and dependent variables. so, it can't be effectively used for non-linear data by definition.\n",
    "* But, we can apply non-linear transformations to the independent variables before using lasso regression. we can include polynomial terms or logarithmic terms which will help in capturing the non-linear relationships between independent and dependent variables\n",
    "* however, if the non-linear relationships are too complex, other techniques such as Polynomial Regression, decision trees, support vector regression should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b616b-86f3-4de0-a614-ff8007b075ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4fa57ea-6734-4638-a079-f10830c1e308",
   "metadata": {},
   "source": [
    "### SOLUTION 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa73e0c-77c9-4830-836c-c32794d75a82",
   "metadata": {},
   "source": [
    "* Lasso uses L1 regularization where the penalty term is proportional to the absolute values of coefficients of independent variables\n",
    "* whereas, Ridge uses L2 regularization where the penalty term is proportional to the square of the coefficients of independent variables.\n",
    "* Also, Lasso aids in feature selection whereas Ridge doens't explicitly performs feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc9f88-d6be-47f0-9a68-b75a62f05642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d9bf65f-55ad-4c55-bf42-ab98bcfd98ff",
   "metadata": {},
   "source": [
    "### SOLUTION 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77ce82-f6cf-4dfd-96b3-b03258bff144",
   "metadata": {},
   "source": [
    "* lasso doesn't handle multicollinearity explicityly like Ridge regression does.\n",
    "* Lasso shrinks the coefficients and performs feature selection.\n",
    "* but in case of multicollinearity, where coefficients are highly related, lasso chooses one variable over the other. so, it leads to variable and unstable feature selection.\n",
    "* also, it doesn't provide any information about the correlation between variables.\n",
    "* so, for explicitly handling multicollinearity, techniques like ridge regression should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7bd93-c27d-45f6-9649-7bf4569bc86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb731cf8-1f9e-4a57-8edf-b6b13c304d10",
   "metadata": {},
   "source": [
    "### SOLUTION 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b7d558-fa6a-40f6-ae31-c30ec4a648b3",
   "metadata": {},
   "source": [
    "the value of lambda can be determined using :\n",
    "- domain knowledge\n",
    "- cross validation\n",
    "- grid search <br>\n",
    "\n",
    "its value has to be such that it optimally balances model performance and handling multicollinearity and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f3d183-0f35-4ffa-a331-143e901019b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
